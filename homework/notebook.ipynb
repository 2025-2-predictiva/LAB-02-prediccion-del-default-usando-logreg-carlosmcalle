{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ca1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2 \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65069fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el orquestador\n",
    "def make_grid_search(estimator, param_grid, cv, scoring='balanced_accuracy', verbose=1, n_jobs=-1):\n",
    "    \"\"\"Paso 4: Crea y ejecuta GridSearchCV.\"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    return grid_search\n",
    "\n",
    "def evaluate_and_save_metrics(model, X_train, y_train, X_test, y_test, output_path):\n",
    "    \"\"\"Paso 6 & 7: Calcula y guarda todas las métricas y CMs en formato JSON por línea.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Métricas para Train\n",
    "    metrics_train = {\n",
    "        'type': 'metrics', \n",
    "        'dataset': 'train', \n",
    "        'precision': precision_score(y_train, y_train_pred), \n",
    "        'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred), \n",
    "        'recall': recall_score(y_train, y_train_pred), \n",
    "        'f1_score': f1_score(y_train, y_train_pred)\n",
    "    }\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_train_dict = format_cm_for_json(cm_train, 'train')\n",
    "\n",
    "    # Métricas para Test\n",
    "    metrics_test = {\n",
    "        'type': 'metrics', \n",
    "        'dataset': 'test', \n",
    "        'precision': precision_score(y_test, y_test_pred), \n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred), \n",
    "        'recall': recall_score(y_test, y_test_pred), \n",
    "        'f1_score': f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    cm_test_dict = format_cm_for_json(cm_test, 'test')\n",
    "    \n",
    "    data_to_save = [metrics_train, metrics_test, cm_train_dict, cm_test_dict]\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data_to_save:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_estimator(estimator, path=\"../files/models/model.pkl.gz\"):\n",
    "    \"\"\"Paso 5: Guarda el modelo comprimido con gzip.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with gzip.open(path, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "    \n",
    "\n",
    "\n",
    "def format_cm_for_json(cm, dataset_name):\n",
    "    \"\"\"Paso 7: Formatea la matriz de confusión según el requisito.\"\"\"\n",
    "    return {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': dataset_name,\n",
    "     \n",
    "        'true_0': {\"predicted_0\": int(cm[0, 0]), \"predicted_1\": int(cm[0, 1])},\n",
    "        'true_1': {\"predicted_0\": int(cm[1, 0]), \"predicted_1\": int(cm[1, 1])}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d574c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos funcion para la limpieza de los datasets\n",
    "\n",
    "def carga_y_limpieza(train_path, test_path):\n",
    "\n",
    "    train_dataset = pd.read_csv(train_path, compression='zip')\n",
    "    test_dataset = pd.read_csv(test_path, compression='zip')\n",
    "        \n",
    "    # Renombrar 'default payment next month' a 'default' y remover 'ID'.\n",
    "    train_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    test_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    train_dataset.drop(columns=[\"ID\"], inplace=True)\n",
    "    test_dataset.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "    #  Agrupar valores > 4 y 0 en EDUCATION en la categoría 'others' (4)\n",
    "    def clean_education(df):\n",
    "        df['EDUCATION'] = np.where(df['EDUCATION'] > 4, 4, df['EDUCATION'])\n",
    "        df['EDUCATION'] = np.where(df['EDUCATION'] == 0, 4, df['EDUCATION'])\n",
    "        return df\n",
    "\n",
    "    train_dataset = clean_education(train_dataset)\n",
    "    test_dataset = clean_education(test_dataset)\n",
    "\n",
    "    #  Eliminar registros con información no disponible (NaN)\n",
    "    train_dataset.dropna(inplace=True)\n",
    "    test_dataset.dropna(inplace=True)\n",
    "\n",
    "    #  Dividir en X e y\n",
    "    X_train = train_dataset.drop(columns=['default'])\n",
    "    y_train = train_dataset['default']\n",
    "    X_test = test_dataset.drop(columns=['default'])\n",
    "    y_test = test_dataset['default']\n",
    "\n",
    "    # Identificar columnas Categóricas y Numéricas\n",
    "\n",
    "    pay_columns = [f\"PAY_{i}\" for i in [0, 2, 3, 4, 5, 6]]\n",
    "    categorical_columns = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"] + pay_columns\n",
    "    \n",
    "    bill_columns = [f\"BILL_AMT{i}\" for i in range(1, 7)]\n",
    "    pay_amt_columns = [f\"PAY_AMT{i}\" for i in range(1, 7)]\n",
    "    numeric_columns = [\"LIMIT_BAL\", \"AGE\"] + bill_columns + pay_amt_columns\n",
    "\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].astype(str) \n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').astype(float)\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').astype(float)\n",
    "            \n",
    "    # dropna para evitar errores\n",
    "    X_train.dropna(inplace=True)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "    X_test.dropna(inplace=True)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "    \n",
    "    print(\" Carga y limpieza completadas. Tipos de datos asegurados.\")\n",
    "    \n",
    "    return X_test, X_train, y_test, y_train, categorical_columns, numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4395e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Carga y limpieza completadas. Tipos de datos asegurados.\n",
      " Pipeline creado y robustecido contra TypeErrors.\n",
      "\n",
      "Iniciando búsqueda de hiperparámetros (GridSearchCV)...\n",
      "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n",
      "\n",
      " Mejor modelo (GridSearch) guardado en 'files/models/model.pkl.gz'\n",
      "\n",
      " Métricas y matrices de confusión guardadas en '../files/output/metrics.json'\n",
      "\n",
      " Entrenamiento y evaluación completados.\n"
     ]
    }
   ],
   "source": [
    "# funcion para entrenamiento\n",
    "\n",
    "def train_and_evaluate_logistic_regression():\n",
    "    \n",
    "    train_path = \"../files/input/train_data.csv.zip\"\n",
    "    test_path = \"../files/input/test_data.csv.zip\"\n",
    "    \n",
    "    X_test, X_train, y_test, y_train, categorical_columns, numeric_columns = carga_y_limpieza(train_path, test_path)\n",
    "\n",
    "    #  Crear el Pipeline\n",
    "    \n",
    "    #  Transformadores\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    # Transformador Categórico: Imputer (para NaN) -> OneHotEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_columns),\n",
    "            ('cat', categorical_transformer, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    #  Pipeline Completo\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocesador', preprocessor), \n",
    "        ('selectkbest', SelectKBest(score_func=chi2)), \n",
    "        ('estimator', LogisticRegression(solver='liblinear', random_state=123, max_iter=5000))\n",
    "    ])\n",
    "    \n",
    "    print(\" Pipeline creado y robustecido contra TypeErrors.\")\n",
    "    \n",
    "    #  Optimización de Hiperparámetros (GridSearch)\n",
    "    param_grid = {\n",
    "        # Ampliar el rango de K para capturar features importantes\n",
    "        'selectkbest__k': [20, 40, 60, 80, 'all'], \n",
    "        # CRÍTICO: Explorar valores más altos de C (menos regularización)\n",
    "        'estimator__C': [0.1, 1, 10, 50, 200, 500], \n",
    "        'estimator__penalty': ['l1', 'l2'], \n",
    "        'estimator__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    grid_search = make_grid_search(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=10, \n",
    "        scoring='balanced_accuracy', \n",
    "        verbose=1,\n",
    "        n_jobs=-1 \n",
    "    )\n",
    "    \n",
    "    print(\"\\nIniciando búsqueda de hiperparámetros (GridSearchCV)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    #  Guardar el mejor modelo\n",
    "    best_model = grid_search\n",
    "    save_estimator(best_model)\n",
    "    print(\"\\n Mejor modelo (GridSearch) guardado en 'files/models/model.pkl.gz'\")\n",
    "\n",
    "    # Calcular y guardar métricas y matrices de confusión\n",
    "    output_path = \"../files/output/metrics.json\"\n",
    "    evaluate_and_save_metrics(best_model, X_train, y_train, X_test, y_test, output_path)\n",
    "    \n",
    "    print(f\"\\n Métricas y matrices de confusión guardadas en '{output_path}'\")\n",
    "    print(\"\\n Entrenamiento y evaluación completados.\")\n",
    "\n",
    "# Ejecutar el codigo \n",
    "if __name__ == '__main__':\n",
    "    train_and_evaluate_logistic_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
